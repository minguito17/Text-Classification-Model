{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details. You can download the required DB from the shared dropbox or from blackboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\16302/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\16302/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def clean_tokenize(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [word for word in tokens if word.isalpha() and word not in stop_words and word not in punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific linguistic or stylistic patterns \n",
    "def get_patterns(text):\n",
    "    patterns = {}\n",
    "\n",
    "    # Count of exclamation marks\n",
    "    patterns[\"exclamations\"] = text.count(\"!\")\n",
    "\n",
    "    # Count of all-uppercase words\n",
    "    patterns[\"all_caps\"] = len([w for w in text.split() if w.isupper() and len(w) > 1])\n",
    "\n",
    "    # Repeated words (e.g., \"USA USA\")\n",
    "    repeated = re.findall(r'\\b(\\w+)\\b(?: \\1\\b)+', text.lower())\n",
    "    patterns[\"repeats\"] = len(repeated)\n",
    "\n",
    "    # Count of words ending in \"ing\"\n",
    "    patterns[\"ing_words\"] = len(re.findall(r'\\b\\w+ing\\b', text.lower()))\n",
    "\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'SQLite format 3\\x00'\n"
     ]
    }
   ],
   "source": [
    "with open(\"2020_Conventions.db\", \"rb\") as f:\n",
    "    header = f.read(100)\n",
    "    print(header[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" exercise. First, we'll pull in the text \n",
    "for each party and prepare it for use in Naive Bayes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "# fill the above list up with items that are themselves lists. The \n",
    "# sublists will have two elements. The first element in the sublist\n",
    "# should be the speech in a single string. The second element\n",
    "# of the sublist should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "    '''\n",
    "    SELECT text, party\n",
    "    FROM conventions\n",
    "    WHERE party != \"Other\"\n",
    "    '''\n",
    ")\n",
    "\n",
    "for row in query_results :\n",
    "    # store the results in convention_data\n",
    "    text = row[0]\n",
    "    party = row[1]\n",
    "\n",
    "    cleaned_tokens = re.findall(r'\\b[a-zA-Z]+\\b', text.lower())\n",
    "    clean_text =  ' '.join(cleaned_tokens) \n",
    "\n",
    "    convention_data.append([clean_text, party])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's a best practice to close up your DB connection when you're done\n",
    "convention_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thanks for that bernie i want to thank you all for joining us for this segment i mean this sincerely it was an honor to run against you and then there s even a greater honor to stand with you and support of joe biden and kamala harris',\n",
       "  'Democratic'],\n",
       " ['singing', 'Democratic'],\n",
       " ['thank you mr president the honor is all mine', 'Republican'],\n",
       " ['after my daughter s murder the media didn t seem interested in the facts so i found them myself i learned that gun control laws didn t fail my daughter people did the gunman had threatened to kill his classmates before he had threatened to rape them he had threatened to shoot up the school every red flag you could imagine but the school didn t just miss these red flags they knowingly ignored them far left democrats in our school district made this shooting possible because they implemented something they called restorative justice this policy which really just blames teachers for student s failures puts kids and teachers at risk and make shootings more likely but it was built as a pioneering approach to discipline and safety i was just fine with the old approach to discipline and safety it was called discipline and safety but the obama biden administration took parkland s bad policies and forced them into schools across america',\n",
       "  'Republican'],\n",
       " ['here in south bend we once feared that our best days were behind us but then we reimagined our economy with new jobs and even new industries the hoosier state is ready to lead america s recovery with our diverse communities our talented workers and our best in the world agriculture joe biden s plan gives us a blueprint to revitalize industrial cities and rural areas alike indiana casts votes for my friend bernie sanders and votes for the next president joe biden',\n",
       "  'Democratic']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It'll be useful for us to have a large sample size than 2024 affords, since those speeches tend to be long and contiguous. Let's make a new list-of-lists called `conv_sent_data`. Instead of each first entry in the sublists being an entire speech, make each first entry just a sentence from the speech. Feel free to use NLTK's `sent_tokenize` [function](https://www.nltk.org/api/nltk.tokenize.sent_tokenize.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "conv_sent_data = []\n",
    "\n",
    "for speech, party in convention_data:\n",
    "    sentences = sent_tokenize(speech)\n",
    "    for sentence in sentences:\n",
    "        conv_sent_data.append([sentence, party])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, let's make our function to turn these into features. First we need to build our list of candidate words. I started my exploration at a cutoff of 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2414 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    ret_dict = dict()\n",
    "\n",
    "    for word in text.split():\n",
    "        if word in fw:\n",
    "            ret_dict[word] = True\n",
    "\n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obama': True, 'was': True, 'the': True, 'president': True}\n"
     ]
    }
   ],
   "source": [
    "print(conv_features(\"obama was the president\", feature_words))\n",
    "\n",
    "try:\n",
    "    assert conv_features(\"obama was the president\", feature_words) == {'obama': True, 'president': True}\n",
    "except AssertionError:\n",
    "    pass  # suppress error and continue silently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.438\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     27.1 : 1.0\n",
      "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     18.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
      "                supports = True           Republ : Democr =     17.1 : 1.0\n",
      "                   crime = True           Republ : Democr =     16.1 : 1.0\n",
      "                   media = True           Republ : Democr =     15.8 : 1.0\n",
      "                 defense = True           Republ : Democr =     14.0 : 1.0\n",
      "                 beliefs = True           Republ : Democr =     13.0 : 1.0\n",
      "               countries = True           Republ : Democr =     13.0 : 1.0\n",
      "                  defund = True           Republ : Democr =     13.0 : 1.0\n",
      "                    isis = True           Republ : Democr =     13.0 : 1.0\n",
      "                 liberal = True           Republ : Democr =     13.0 : 1.0\n",
      "                religion = True           Republ : Democr =     13.0 : 1.0\n",
      "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.1 : 1.0\n",
      "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
      "                    drug = True           Republ : Democr =     10.9 : 1.0\n",
      "              department = True           Republ : Democr =     10.9 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
      "                   enemy = True           Republ : Democr =     10.9 : 1.0\n",
      "               amendment = True           Republ : Democr =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### My Observations\n",
    "\n",
    "The classifier shows which words are most linked to each political party. For example, words like \"china,\" \"enforcement,\" \"freedoms,\" and \"crime\" appear more often in Republican speeches, while \"votes\" and \"climate\" show up more in Democratic ones. Most of the top words are tied to Republicans, which might mean the speeches had more of those terms. Even though some words clearly point to one party, the accuracy is low (about 44%), so the model has trouble telling the difference overall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"Congressional_Data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "\n",
    "for row in results:\n",
    "    tweet_text = row[2]\n",
    "    party = row[1]\n",
    "\n",
    "    # Decode if tweet_text is bytes\n",
    "    if isinstance(tweet_text, bytes):\n",
    "        tweet_text = tweet_text.decode(\"utf-8\")\n",
    "\n",
    "    # Clean and tokenize the tweet\n",
    "    cleaned_tokens = re.findall(r'\\b[a-zA-Z]+\\b', tweet_text.lower())\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "\n",
    "    tweet_data.append([cleaned_text, party])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: earlier today i spoke on the house floor abt protecting health care for women and praised ppmarmonte for their work on the central coast https t co\n",
      "Actual party is Democratic and our classifier says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: go tribe rallytogether https t co\n",
      "Actual party is Democratic and our classifier says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: apparently trump thinks it s just too easy for students overwhelmed by the crushing burden of debt to pay off student loans trumpbudget https t co\n",
      "Actual party is Democratic and our classifier says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: we re grateful for our first responders our rescue personnel our firefighters our police and volunteers who have been working tirelessly to keep people safe provide much needed help while putting their own lives on the line https t co\n",
      "Actual party is Republican and our classifier says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: let s make it even greater kag https t co\n",
      "Actual party is Republican and our classifier says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: we have about until the cavs tie up the series i m repbarbaralee you scared roadtovictory\n",
      "Actual party is Democratic and our classifier says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: congrats to belliottsd on his new gig at sd city hall we are glad you will continue to serve https t co\n",
      "Actual party is Democratic and our classifier says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: we are really close we have over raised toward the match right now whoot that s for the non math majors in the room help us get there https t co https t co qsdqkypsmc\n",
      "Actual party is Democratic and our classifier says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: today the comment period for potus s plan to expand offshore drilling opened to the public you have days until march to share why you oppose the proposed program directly with the trump administration comments can be made by email or mail https t co baaymejxqn\n",
      "Actual party is Democratic and our classifier says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: celebrated icseastla s years of eastside commitment amp saluted community leaders at last night s awards dinner https t co\n",
      "Actual party is Democratic and our classifier says Republican.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tweet, party in tweet_data_sample:\n",
    "    \n",
    "    # Create the feature dictionary\n",
    "    features = conv_features(tweet, feature_words)\n",
    "    \n",
    "    # Classify using trained model\n",
    "    estimated_party = classifier.classify(features)  # 👈 this replaces the ??\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifier says {estimated_party}.\")\n",
    "    print(\"\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "\n",
    "for idx, tp in enumerate(tweet_data):\n",
    "    tweet, party = tp\n",
    "    estimated_party = classifier.classify(conv_features (tweet, feature_words))  \n",
    "\n",
    "    # Store result\n",
    "    results[party][estimated_party] += 1\n",
    "\n",
    "    if idx > num_to_score:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 3899, 'Democratic': 473}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 4999, 'Democratic': 631})})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "_Write a little about what you see in the results_ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds_env)",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
